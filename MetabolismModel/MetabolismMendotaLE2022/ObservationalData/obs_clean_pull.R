library(tidyverse)
library(dplyr)
library(lubridate)
library(zoo)

# TRIED TO DO THERMOCLINE DEPTH CALCULATION....FAILED

# drv <- dbDriver("PostgreSQL") ##this is where you call the name of your ODBC connection
# 
# ##create connection (I use the server id here but the url name works too)
# con <- dbConnect(drv,
#                  dbname = "UW_data_science",
#                  host = '144.92.62.199',
#                  port = 5432,
#                  user = "postgres",
#                  password = 'MononaSturgeon')
# 
# ## pull in temp data from Jordan (postgres) -- (1995 - 2019)
# nhdid <- 'nhdhr_143249470'
# 
# min_date <- '1995-01-01'
# max_date <- '2019-12-31'
# #max_date <- '2010-12-31'
# 
# data_query <- paste('select * from data.predicted_temps_calibrated where "nhd_lake_id" = \'',nhdid,'\' and date >= \'',min_date,'\'',' and date <= \'',max_date,'\'', sep = '')
# 
# shape_query <- paste('select * from data.lake_metrics where "nhd_lake_id" = \'',nhdid,"'", sep = '')
# 
# lake_data <- dbGetQuery(con,data_query, stringsAsFactors = FALSE)
# lake_morphology <- dbGetQuery(con,shape_query, stringsAsFactors = FALSE)
# 
# dbDisconnect(con)

# 
# #####READ IN ROBERT'S THERMOCLINE DEPTH ASSIGNMENT CODE#########
# source("~/Documents/masters_work/model/metab_model/td_assign_smooth.R", echo = FALSE)
# 
# ####Thermocline depth calculation code taken from Robert's ODEM model#######
# 
# data = me_temp_df ##need original data format
# p.data <- c()
# 
# for (ki in sort(unique(data$date))){
#   yi <- which(data$date == ki)
#   #p.data <- rbind(p.data, rev(data$Value[yi]))
#   p.data <- rbind(p.data, rev(data$Value[yi]))
#   
# }
# 
# data2 <- as.data.frame(cbind(rep(1, length(unique(data$date))), p.data))
# colnames(data2) <- c('date',paste0('temp_',sort(unique(data$Depth)[1:25])))
# data2$date = as.Date(sort(unique(data$date)))
# 
# 
# td_df <- data.frame(calc_td_depth(data2)) ##list of td values
# td_df$sampledate <- as.Date(sort(unique(data$date)))
# td_df <- td_df %>% rename(thermocline_depth = calc_td_depth.data2.)
# 
# ### SMOOTH THERMOCLINE DEPTH WITH MOVING AVERAGE
# 
# ##no volume calculation yet so can't compute meandepth -- use half of lake depth
# max_depth <- lake_nml$morphometry$H[length(lake_nml$morphometry$H)] - lake_nml$morphometry$H[1]
# 
# td_default <- max_depth/2
# early_strat_value <- 5
# 
# td_df <- td_df %>% 
#   mutate(month = month(sampledate))
# 
# td_df$td_roll <- rollmean(td_df$thermocline_depth, k=14, fill = NA)
# 
# ##padding for early stratification
# td_df$td_roll <- ifelse(!(is.na(td_df$thermocline_depth)) & is.na(td_df$td_roll) & td_df$month < 7,early_strat_value,td_df$td_roll)
# 
# ##paddding before mixing 
# td_df$td_roll <- ifelse(!(is.na(td_df$thermocline_depth)) & is.na(td_df$td_roll) & td_df$month > 7,td_default,td_df$td_roll)
# 
# 
# td_df$thermocline_depth <- td_df$td_roll
# td_df$td_roll <- NULL
# td_df$month <- NULL
# 
# ## implement minimum layer depth for epi and hypo (2m limit)
# td_df$thermocline_depth <- ifelse(td_df$thermocline_depth < 2, 2, td_df$thermocline_depth) #epi check 
# td_df$thermocline_depth <- ifelse((max_depth - td_df$thermocline_depth) < 2, max_depth - 2, td_df$thermocline_depth) #hypo check
# 
# 
# td_assign <- lake_date_fix %>% right_join(td_df, by = c('sampledate')) ## rejoin onto data with corrected dates
# 
# 
# 
# 
# 
# 
# ## potential workflow
# 
# #Jordan's thermal profiles (through 2019)
# #might need to supplement with LTER wtemp data
# 
# #check with Paul about the current obs table he shared....this was not generated by me
# 

# JUST USE OBSERVATIONAL DATA AND ASSUME THAT 2M DEPTH IS EPI, 20M DEPTH IS HYPO
# JUST NEED DATA FOR 2021
# Grab DO/Temp data
obs_df <- read_csv('~/Documents/masters_work/for_paul/grad_cohort_data/obs/ntl29_v11.csv')
obs_template <- read_csv('~/Documents/masters_work/for_paul/grad_cohort_data/obs/Mendota_observations_template.csv')

obs_template$sampledate <- as.Date(obs_template$sampledate)

obs_template_doc_check <- obs_template %>% #used later on for check -- might delete this process
  filter(sampledate > '2018-12-31')

me_obs_do_temp <- obs_df %>% 
  filter(lakeid == 'ME') %>% #Mendota
  rename(year = year4) %>% 
  filter(sampledate >= '2021-01-01')  %>% 
  filter(depth %in% c(4,20)) %>% # we chose depths for epi = 4 and hypo = 20
  rename(temp = wtemp) %>% 
  mutate(layer = ifelse(depth == 4,'epi','hypo')) %>% #assign layer based off of depth value (4m = epi)
  select(sampledate,year,daynum,layer,temp,o2) %>% 
  mutate(thermocline = NA) %>% #create extra columns from Paul's template
  mutate(stratFlag = NA) %>% 
  mutate(tp = NA) %>% 
  mutate(doc = NA)

me_obs_do_temp$sampledate <- as.Date(me_obs_do_temp$sampledate)

# NEED TO ADD AVERAGE VALUES OF TEMP AND DO FOR "ALL" LAYER VALUES
me_obs_do_temp_all_layer <- me_obs_do_temp
me_obs_do_temp_all_layer$layer <- 'all'

me_obs_do_temp_all_layer_avg <- me_obs_do_temp_all_layer %>% 
  group_by(sampledate) %>% 
  mutate(avg_val_do = mean(o2,na.rm = TRUE)) %>% 
  mutate(avg_val_temp = mean(temp,na.rm = TRUE)) %>% 
  ungroup() %>% 
  distinct(sampledate, .keep_all = TRUE)

me_obs_do_temp_all_layer_avg$o2 <- me_obs_do_temp_all_layer_avg$avg_val_do
me_obs_do_temp_all_layer_avg$temp <- me_obs_do_temp_all_layer_avg$avg_val_temp

me_obs_do_temp_all_layer_avg <- me_obs_do_temp_all_layer_avg %>% select(-avg_val_do,-avg_val_temp)

me_obs_do_temp_all <- rbind(me_obs_do_temp,me_obs_do_temp_all_layer_avg)


# grab updated doc data 
doc_updated <- read_csv('~/Documents/masters_work/for_paul/grad_cohort_data/obs/ME_doc_updated.csv')
doc_updated$sampledate <- as.Date(doc_updated$sampledate)

me_obs_doc <- doc_updated %>% 
  filter(depth %in% c(4,20)) %>% # same depth selection as above 
  mutate(layer = ifelse(depth == 4,'epi','hypo')) %>% 
  distinct(sampledate,layer, .keep_all = TRUE) %>% 
  select(sampledate,layer,doc) %>% 
  rename(doc_updated = doc)


me_doc_join <- obs_template %>% left_join(me_obs_doc, by = c('sampledate','layer')) # join new doc data onto existing data based off of date and layer

me_doc_join$doc <- ifelse(!is.na(me_doc_join$doc_updated),me_doc_join$doc_updated,me_doc_join$doc) #assign new values into existing column


me_doc_join <- me_doc_join %>% 
  group_by(sampledate) %>% 
  mutate(g_count = n()) %>% #count number of observations per group
  mutate(doc_avg = ifelse(g_count >=2,mean(doc,na.rm = TRUE),NA)) %>% # if both more than one samples exists (epi/hypo) than take an average
  ungroup()

# assign new doc values for "all" layer value -- calculated above
# assigment made on three criteria: updated doc data (post 2018), "all" layer value, NA value already in the all column (redundant)
me_doc_join$doc <- ifelse(me_doc_join$sampledate > '2018-12-31' & me_doc_join$layer == 'all' & is.na(me_doc_join$doc),
                          me_doc_join$doc_avg, 
                          me_doc_join$doc)

me_template_updated <- me_doc_join %>% select(-g_count,-doc_avg,-doc_updated)





##combine the updated "template" df (data through 2020) with the new do/temp data (through 2021)
me_all_df <- rbind(me_template_updated,me_obs_do_temp_all)
me_all_df$doc[is.nan(me_all_df$doc)]<-NA #fix NaN values

write.csv(me_all_df,'~/Documents/masters_work/for_paul/grad_cohort_data/obs/ME_observations_updated.csv', row.names = FALSE)
